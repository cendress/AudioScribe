# AudioScribe ðŸ““ðŸŽ™ï¸  
iOS voiceâ€‘notes recorder built with SwiftUI and SwiftData.  
Runs happily in the background, survives interruptions, and shows a live audio waveform.

---

## Feature HighlightsÂ 

| Capability |  |
|----|------------|
| Oneâ€‘tap **RecordÂ /Â PauseÂ /Â Stop** |  âœ”ï¸Ž
| Autoâ€‘resume after phone calls or Siri |  âœ”ï¸Ž
| Live audio **waveform meter** (RMS) |  âœ”ï¸Ž
| **Session list** with search and infinite scroll |  âœ”ï¸Ž
| **Session detail** screen (perâ€‘segment placeholder) |  âœ”ï¸Ž
| Scalable **SwiftData** store â€“ >â€¯1â€¯k sessions / 10â€¯k segments |  âœ”ï¸Ž
| VoiceOverâ€‘friendly labels |  âœ”ï¸Ž

> Whisper STT, widgets, and iCloud sync are on the roadmap â€“ see *docs/KNOWN_ISSUES.md*.

---

## Screenshots

| Recorder | Session List | Session Detail |
|----------|--------------|----------------|
|  |  |  |

---

## QuickÂ Start

1. Clone the repository  
Â Â Â `git clone https://github.com/yourâ€‘handle/AudioScribe.git`  
Â Â Â `cd AudioScribe`
2. Open **AudioScribe.xcodeproj** in XcodeÂ 16 or newer.
3. Build & run on a *physical* device (simulator lacks a mic).
4. Grant microphone permission, press **Record**, lock the phone, recording continues in the background.

---

## Architecture  
AudioScribe uses a Cleanâ€‘Architecture, **Modelâ€‘Viewâ€‘ViewModel (MVVM)** approach wellâ€‘suited to SwiftUIâ€™s dataâ€‘binding and Combineâ€‘driven state management.

### Key Components:
- **Model (`RecordingSession`, `Segment`, `Transcription`)**  
  Plain Swift types that represent a recording session, its 30â€‘second audio chunks, and transcript text. 

- **ViewModel (`RecorderViewModel`, `SessionListViewModel`, `SessionDetailViewModel`)**  
  Act as the intermediaries between views and business logic. Each viewâ€‘model publishes UI state (e.g., recording status, waveform level, sessions), processes user intents (record, pause, search), and communicates with services solely through protocols, making them fully mockable for unit testing.

- **View (`RecorderView`, `SessionListView`, `SessionDetailView`)**  
  SwiftUI screens responsible for layout, navigation, and accessibility. They observe their corresponding viewâ€‘models (`@StateObject` / `@ObservedObject`) and contain no business logic, keeping the UI layer thin and declarative.

- **Services**  
  Concrete implementations that fulfill the domain protocols while encapsulating heavy tasks.  
  * **Audio Services** â€“ `AVAudioEngineRecorder`, `AudioSessionManager`, `RecordingCoordinator` manage microphone capture, audioâ€‘session interruptions, and onâ€‘theâ€‘fly persistence of new segments.  
  * **Transcription Services** â€“ `TranscriptionManager`, `WhisperTranscriptionService`, `LocalTranscriptionService` will upload 30â€‘second chunks, retry failed jobs, and fall back to onâ€‘device speech recognition.  
  * **Persistence** â€“ `DataController` sets up and maintains the SwiftData container.

- **Helpers (`DiskSpaceMonitor`, `CryptoHelper`, `KeychainStore`)**  
  Stateless utility types that handle crossâ€‘cutting concernsâ€”freeâ€‘space checks, AES encryption, secure token storageâ€”allowing core components to stay focused on a single responsibility.

This structure keeps dependencies flowing in one direction (UI â†’ ViewModel â†’ Protocols â†’ Services), ensuring each layer can evolve or be tested independently without leaking implementation details across boundaries.

---

## Design Notes

* **Clean Architecture** keeps UI, domain protocols, and AVFoundation code separate.
* **Protocols everywhere** â€“ makes audio and data layers mockable for tests.
* **SwiftData and Combine** supply live updates so list UIs stay tiny.
* **Accelerate** powers the waveform with minimal CPU (<â€¯3â€¯% on A15).

---

## Roadmap

* Noiseâ€‘reduction DSP and homeâ€‘screen widget  
* iCloud / CloudKit sync
* Further tests for full integration

See full list in *docs/KNOWN_ISSUES.md*.

---

## License

MIT Â©Â 2025Â ChristopherÂ Endress
